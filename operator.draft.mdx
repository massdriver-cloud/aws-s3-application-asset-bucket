
# Intention/Use-Cases

## Asset Serving for API

Many APIs need persistent blob storage for artifacts / manifests / objects that aren&#39;t very valuable sitting on disk in a database. Examples include large blobs that may be accepted from the client in an unstructured format or a format not well understood by your data model (e.g. storing yaml in a string field is not usually very useful). Having a bucket to store / retrieve these assets can allow for keeping historical versions of large blobs with less cost than storing these blobs in the database. There is obviously a latency penalty in retrieving objects from s3 so this is not suitable for low latency use cases, but does make a lot of sense of larger payloads, or archives (only keep the latest version in the DB but back up the historical info to s3 just in case you need to roll back). A concrete example of this might be an unstructured file store to back a SaaS product like spreadsheets to back airtable.

## ML Model Storage

This is a special flavor of the above use case where model data is stored / loaded form s3 in an API that uses this information to serve a machine learning model. A concrete example would be storing (historical versions) TF2 (tensorflow model storage format) holding the weights the attributes for your content based filtering model. This might allow for A / B testing (and roll back) of user engagement with updates to a recommendations algorithm.

## 

# Non-intentions / Out of Scope Use-cases  (for this bundle)

- Eventing bucket for lambda ETL (would include notifications)
- Static Website Content (would include routing / endpoint configuration)
- Cold / Archival storage only
- Data Lake (analytics configuration and option for transfer accelerate configuration)
- Replication to other regions
- Requester Pays user content download
- Log Storage
    - Many distributed apps flush their logs (potentially of varying structures) to a bucket that a separate service does analytics on or provides a GUI / filtering interface over. A concrete example could be logs from every smart TV (consumers may have different software versions sending different structure of logs) and these ultimately get dumped to a bucket where an analytics pipeline can see common patterns of crash / error logs and the preceding events. Additionally a support team query service could retrieve logs for a particular device from the same bucket when troubleshooting an individual user&#39;s pain.

# Resources

- `aws_kms_key`
- `aws_s3_bucket` (aws manages a lot of configs to do with buckets as separate resources rather than nested config blocks so all the following are related to this bucket)
    - `aws_s3_bucket_policy`
    - `aws_s3_bucket_public_access_block` (block public ACLs / policies)
    - `aws_s3_bucket_server_side_encryption_configuration`  

# Design

This bucket will not be configured with object versioning enabled. In most use cases it seems Write Once is probably preferred.


# Configuration

## Params

- AWS Region



## Connections

- `aws-iam-role`  
- (not a network bound resource)

## Artifacts

- `aws-s3-bucket`
    - ARN 
    - `aws-iam-role` (read, write)

# Best Practices

- Deploys regional S3 for High availability in the event of zonal failure
- Uses a dedicated KMS key for encryption

# Security

- Encrypted by default with dedicated KMS key.
- Private ACL
